{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Report \n",
    "\n",
    "\n",
    "\n",
    "## Background\n",
    "\n",
    "Social media has become a new way of communication. It has revolutionized the relationship between users and digital products. The social media users are not only consumers, they are also content (comments) creators and spreaders. A massive amount of data has been generated directly by users and this allows us to measure people’s attention and attitude regarding a product in large scale. \n",
    "\n",
    "However, can the social media activities reflect our behaviors in real life?  Previous research showed that the tweets in a critical time period can successfully predict real world outcomes. A study on movie showed that both tweet - rate (number of tweets per hour) and sentiments could predict the box-office revenue (Asur, 2010) and the rating (Oghina,2012) of a particular movie. Also, the overall attitude of tweets has high correlations with people’s behavior in the stock market (Bollen,2011), political elections (Bermingham,2011), and etc.,\n",
    "\n",
    "However, it may go beyond our expectations in the ways how these social media indicators are correlated with the real world outcomes. For example, products (De Vries,2012) and movies (e.g., [Tiny Times](https://en.wikipedia.org/wiki/Tiny_Times)) that have received negative comments were even more popular than those who received relatively higher ratings in social media. \n",
    "In this project, we are interested in how social media is related with users’ behavior in recreational activities. Also, we are curious about how the accuracy affected by factors such as the popularity of the social media and the diversity of information sources. \n",
    "\n",
    "Our hypothesis are:\n",
    "- Both attention and attitude can predict the outcomes. Polarity will cause more attention \n",
    "- The popularity of social media and the diversity of information sources positively related with the accuracy.\n",
    "\n",
    "In this project, we use movie as our subject area because:\n",
    "- This subject area has research basis, which is both a good foundation of our project and a credible resource for comparing the project results.\n",
    "- The real world outcomes (purchase behavior) can be easily measured by the box-office revenue.\n",
    "- The “quality” of a product can be indicated by the IMDb score.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-preparation & analysis\n",
    "\n",
    "### 1. Movie Box Office Data  \n",
    "   \n",
    "   In the data preparation section, the first step of the project is to grasp movie data and twitter data separately:\n",
    "For the movie data part, the main information we care about are movie’s name, total domestic box office amount (we narrow down the research area for only focus on North America region), movie’s release date, genre, and distributors. The first two features are the main identifier for the movie marked as our label. The third feature (release date) is used for targeting a specific time range for tweets search, and the rest of the features can be used for movie classification since we assume that correlations between twitter and a movie’s box office may also depends on movie’s type.\n",
    "\n",
    "During the searching for box office data, we found that most of the movie box office information is either not complete or not freely available. There is no public free API available for large scale movie box office query. Therefore, we decide to write web scrapper by ourselves to collect box office data from [Boxoffice-Mojo](www.boxofficemojo.com). Boxoffice-Mojo is a website that tracks box office for more than 16,000 movies, basically, we use [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/) and python [urllib2](https://docs.python.org/2/library/urllib2.html) library  to do web data scrapping and saved the data into ‘movies_data.pkl’ file. \n",
    "\n",
    "In the following code, movie data is exported and sorted by its domestic total gross in decreasing order. The top 5 ranking movies are shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import unirest\n",
    "import nltk\n",
    "import sklearn\n",
    "import string\n",
    "import csv\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from pattern.en import sentiment, parsetree\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "from get_tweets import get_tweets\n",
    "from text_classification import process_all, get_rare_words, create_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('movies_data1.pkl', 'r') as picklefile:\n",
    "    movies_scraped, movies_skipped = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies = pd.DataFrame(movies_scraped)\n",
    "movies.dropna(axis=0, subset=['domestic_total_gross'], inplace=True)\n",
    "movies.sort_values(by='domestic_total_gross', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14682\n",
      "BOM_id                          object\n",
      "actors                          object\n",
      "budget                         float64\n",
      "director                        object\n",
      "distributor                     object\n",
      "domestic_total_gross           float64\n",
      "genre                           object\n",
      "movie_title                     object\n",
      "opening_income_wend            float64\n",
      "opening_theaters               float64\n",
      "rating                          object\n",
      "release_date            datetime64[ns]\n",
      "runtime_mins                   float64\n",
      "dtype: object\n",
      "avatar\n",
      "jurassicpark4\n",
      "avengers11\n",
      "titanic\n",
      "darkknight\n",
      "pixar2015\n",
      "avengers2\n",
      "batman3\n",
      "shrek2\n"
     ]
    }
   ],
   "source": [
    "print len(movies)\n",
    "print movies.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 14,682 movies’ information is dumped from the website and we checked our data by sorting their domestic total box office in descending order. We can see that the top 5 movies are:\n",
    "* Star War VII        : 936,662,225 \n",
    "* Avatar              : 749,766,139\n",
    "* Jurassic Park IV    : 652,270,625\n",
    "* Avengers II         : 623,357,910\n",
    "* Titanic             : 600,788,188\n",
    "\n",
    "It is already verified by other resource which means our data is reliable.\n",
    "Here is the genre information for the collected movies and their counting distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Sci-Fi Fantasy' u'Sci-Fi Adventure' u'Action / Adventure' u'Romance'\n",
      " u'Animation' u'Action Thriller' u'Period Adventure' u'Sci-Fi Action'\n",
      " u'Fantasy' u'Historical Drama' u'Adventure' u'Action' u'Family Adventure'\n",
      " u'Sci-Fi Horror' u'Drama' u'Comedy / Drama' u'Horror' u'Family Comedy'\n",
      " u'Comedy' u'Sci-Fi Thriller' u'Horror Thriller' u'Sports Drama'\n",
      " u'Sci-Fi Comedy' u'Fantasy Comedy' u'Action Drama' u'Romantic Comedy'\n",
      " u'Action Comedy' u'Horror Comedy' u'Sci-Fi' u'Fantasy Drama' u'Thriller'\n",
      " u'War' u'Period Action' u'Action Horror' u'Historical Epic' u'Western'\n",
      " u'Crime Comedy' u'Adventure Comedy' u'Period Drama' u'Musical'\n",
      " u'Sports Comedy' u'Drama / Thriller' u'Crime Drama' u'Foreign / Action'\n",
      " u'Period Horror' u'Music Drama' u'Western Comedy' u'Documentary'\n",
      " u'War Drama' u'Sports Action' u'Period Comedy' u'Crime' u'Action / Crime'\n",
      " u'War Romance' u'IMAX' u'Action Fantasy' u'Crime Thriller'\n",
      " u'Romantic Thriller' u'Comedy Thriller' u'Family' u'Romantic Adventure'\n",
      " u'Concert' u'Foreign' u'Foreign / Horror' u'Unknown' u'Sports']\n",
      "Unknown               2065\n",
      "Foreign               1671\n",
      "Drama                 1524\n",
      "Documentary           1512\n",
      "Comedy                1359\n",
      "Comedy / Drama         457\n",
      "Animation              451\n",
      "Romantic Comedy        451\n",
      "Horror                 430\n",
      "Thriller               419\n",
      "Action                 290\n",
      "Romance                250\n",
      "Drama / Thriller       214\n",
      "Crime Drama            186\n",
      "Period Drama           169\n",
      "Action Comedy          167\n",
      "Family Comedy          158\n",
      "Family Adventure       142\n",
      "Music Drama            132\n",
      "Sports Drama           121\n",
      "Action Thriller        115\n",
      "Action / Adventure     114\n",
      "Crime Comedy           112\n",
      "Horror Comedy          111\n",
      "Crime Thriller          94\n",
      "Fantasy                 94\n",
      "Horror Thriller         92\n",
      "Musical                 92\n",
      "Foreign / Action        89\n",
      "Sci-Fi Action           84\n",
      "                      ... \n",
      "Sci-Fi Horror           65\n",
      "Sci-Fi Adventure        63\n",
      "Fantasy Drama           59\n",
      "War Drama               58\n",
      "Period Adventure        54\n",
      "Sci-Fi Thriller         53\n",
      "Romantic Thriller       51\n",
      "Action / Crime          50\n",
      "Adventure               49\n",
      "Concert                 49\n",
      "Period Comedy           48\n",
      "Historical Drama        48\n",
      "War                     47\n",
      "Sci-Fi Comedy           47\n",
      "Sci-Fi                  44\n",
      "Action Horror           40\n",
      "Comedy Thriller         40\n",
      "Crime                   39\n",
      "Foreign / Horror        29\n",
      "Adventure Comedy        23\n",
      "Sci-Fi Fantasy          19\n",
      "Period Action           18\n",
      "Historical Epic         17\n",
      "Action Fantasy          16\n",
      "Western Comedy          14\n",
      "Period Horror           14\n",
      "Sports Action           12\n",
      "War Romance              5\n",
      "Romantic Adventure       4\n",
      "Sports                   1\n",
      "Name: genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print movies['genre'].unique()\n",
    "print movies['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Twitter comments data\n",
    "  \n",
    "  When collecting twitter data, we also met some challenges. Our task is to collect tweets related to a specific film during the time period of the film release. By ‘related to a film’, we use keyword and hashtag search within one tweet. If one tweet contains a film’s name or hash-tag, we regard this tweet as related to this film. Unfortunately, Twitter [streaming API](https://dev.twitter.com/streaming/overview) only allows us to fetch the latest tweets within a week when given keywords and hashtag. Therefore, most of the films’ related tweets which posted earlier than a week cannot be fetched. We also tried several reliable open-sourced twitter data fetching library, since most of them are based on Twitter streaming API so they cannot satisfy our requirement. One of a not-very-popular library is found [here](https://github.com/Jefferson-Henrique/GetOldTweets-python). The basic idea is that when you enter on Twitter page a scroll loader starts, if you scroll down you start to get more and more tweets, all through calls to a JSON provider. After mimic we get the best advantage of Twitter Search on browsers, it can search the deepest oldest tweets. The good thing is that this library can fit most of our requirement, and the bad thing is that it is not a very reliable library due to its popularity, but so far we do not find any side effect or flaws when using this library.\n",
    " \n",
    "Blocks below are codes we used to collect tweets data. Temporarily, we set the max tweets number for each movie to be 1,000. We will adjust this number in the future if the training/testing performance does not go well. Basic features we used for each tweet are its word count, retweet number, and whether it contains links. Besides, we conduct sentimental analysis from a [public API](http://text-processing.com/api/sentiment/). Due to the daily usage limitation for sentimental analysis API, we only choose seven films to do the sentimental analysis here, among the seven movies, four of them are popular movies which have very high box office,while the other threes are relatively normal movies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_range = datetime.timedelta(days=45)\n",
    "maxtweets = 1000\n",
    "\n",
    "def fetch_tweets(row):\n",
    "    BOM_id = row['BOM_id']\n",
    "    movie_title = row['movie_title']\n",
    "    query = BOM_id + ' OR #' + BOM_id + ' OR ' + movie_title + ' OR #' + movie_title\n",
    "    date = row['release_date']\n",
    "    start = date.date().strftime('%Y-%m-%d')\n",
    "    end = (date.date() + time_range).strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        get_tweets(filename='data2/' + BOM_id, maxtweets=maxtweets, query=query, since=start, until=end)\n",
    "        print movie_title, start, end\n",
    "    except:\n",
    "        print \"Problem with fetching tweets of\" + BOM_id\n",
    "        return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Output file generated \"data2/americanbeauty\".\n",
      "American Beauty 1999-09-15 1999-10-30\n",
      "Done. Output file generated \"data2/officerandagentleman\".\n",
      "An Officer and a Gentleman 1982-07-30 1982-09-13\n",
      "Done. Output file generated \"data2/ring\".\n",
      "The Ring 2002-10-18 2002-12-02\n",
      "Done. Output file generated \"data2/nuttyprofessor\".\n",
      "The Nutty Professor 1996-06-28 1996-08-12\n",
      "Done. Output file generated \"data2/borat\".\n",
      "Borat: Cultural Learnings of America for Make Benefit Glorious Nation of Kazakhstan 2006-11-03 2006-12-18\n",
      "Done. Output file generated \"data2/ghostbusters2016\".\n",
      "Ghostbusters 2016-07-15 2016-08-29\n",
      "Done. Output file generated \"data2/secretservice\".\n",
      "Kingsman: The Secret Service 2015-02-13 2015-03-30\n",
      "Done. Output file generated \"data2/robots\".\n",
      "Robots 2005-03-11 2005-04-25\n",
      "Done. Output file generated \"data2/comingtoamerica\".\n",
      "Coming to America 1988-06-29 1988-08-13\n",
      "Done. Output file generated \"data2/crouchingtigerhiddendragon\".\n",
      "Crouching Tiger, Hidden Dragon 2000-12-08 2001-01-22\n",
      "Done. Output file generated \"data2/shutterisland\".\n",
      "Shutter Island 2010-02-19 2010-04-05\n",
      "Done. Output file generated \"data2/intothewoods\".\n",
      "Into the Woods 2014-12-25 2015-02-08\n",
      "Done. Output file generated \"data2/rocky4\".\n",
      "Rocky IV 1985-11-27 1986-01-11\n",
      "Done. Output file generated \"data2/enchanted\".\n",
      "Enchanted 2007-11-21 2008-01-05\n",
      "Done. Output file generated \"data2/curiouscaseofbenjaminbutton\".\n",
      "The Curious Case of Benjamin Button 2008-12-25 2009-02-08\n",
      "Done. Output file generated \"data2/centralintelligence\".\n",
      "Central Intelligence 2016-06-17 2016-08-01\n",
      "Done. Output file generated \"data2/sweethomealabama\".\n",
      "Sweet Home Alabama 2002-09-27 2002-11-11\n",
      "Done. Output file generated \"data2/dumbanddumber\".\n",
      "Dumb and Dumber 1994-12-16 1995-01-30\n",
      "Done. Output file generated \"data2/2fast2furious\".\n",
      "2 Fast 2 Furious 2003-06-06 2003-07-21\n",
      "Done. Output file generated \"data2/mybestfriendswedding\".\n",
      "My Best Friend's Wedding 1997-06-20 1997-08-04\n"
     ]
    }
   ],
   "source": [
    "# fetched index: 0 ~ 20, 200 ~ 220, 400 ~ 420\n",
    "start = 400 \n",
    "end = 420\n",
    "\n",
    "for _, row in movies.iloc[start:end].iterrows():\n",
    "    fetch_tweets(row)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/avatar', 'data/avengers11', 'data/jurassicpark4', 'data/misterlonely', 'data/starwars7', 'data/totalrecall2012rerelease', 'data/womenintrouble']\n",
      "avatar\n",
      "avengers11\n",
      "jurassicpark4\n",
      "misterlonely\n",
      "starwars7\n",
      "totalrecall2012rerelease\n",
      "womenintrouble\n"
     ]
    }
   ],
   "source": [
    "# fetched_movie = {}\n",
    "# directories = glob('data/*')\n",
    "# print directories\n",
    "# for directory in directories:\n",
    "#     BOM_id = directory[5:]\n",
    "#     print BOM_id\n",
    "#     fetched_movie[BOM_id] = pd.read_csv(directory, sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def sentiment(text):\n",
    "#     # response = requests.post('http://text-processing.com/api/sentiment/', data={'text': text})\n",
    "#     response = unirest.post(\"https://japerk-text-processing.p.mashape.com/sentiment/\",\n",
    "#         headers={\n",
    "#             \"X-Mashape-Key\": \"EMrBfg9GO4mshgbevq2BtBZCdet3p1iXIWejsnKRDuWRNljYxI\",\n",
    "#             \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "#             \"Accept\": \"application/json\"\n",
    "#           },\n",
    "#         params={\n",
    "#             \"language\": \"english\",\n",
    "#             \"text\": text\n",
    "#           }\n",
    "#     )\n",
    "#     if response.code != 200:\n",
    "#         print response.code\n",
    "#         print response.body\n",
    "#         print 'failed: {}'.format(text)\n",
    "#         return {'sentiment': None, 'probability': None}\n",
    "#     sentiment = response.body['label']\n",
    "#     if sentiment == 'pos':\n",
    "#         return {'sentiment': 1, 'probability': response.body['probability']}\n",
    "#     elif sentiment == 'neg':\n",
    "#         return {'sentiment': -1, 'probability': response.body['probability']}\n",
    "#     else:\n",
    "#         return {'sentiment': 0, 'probability': response.body['probability']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def get_sentiments(movie_dict):\n",
    "#     for BOM_id, movie in movie_dict.items():\n",
    "#         sentiments = pd.DataFrame([sentiment(text) for text in movie['text']])\n",
    "#         movie_dict[BOM_id] = pd.concat([movie, sentiments], axis=1)\n",
    "#         #movie.to_csv('data/' + BOM_id, sep=';')\n",
    "#         print BOM_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def ouput_csv(movie_dict):\n",
    "#     for BOM_id, movie in movie_dict.items():\n",
    "#         movie.to_csv('data/' + BOM_id, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following result we can see that, according to the sample we collected, popular movies tend to have more positive reviews and more tweets and retweets amount as a whole. More detailed research and their correlation will be studied after midterm report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starwars7  :  Counter({0.0: 650, 1.0: 335, -1.0: 11, nan: 1, nan: 1}) total:  998\n",
      "avengers11  :  Counter({1: 777, 0: 183, -1: 39}) total:  999\n",
      "womenintrouble  :  Counter({0: 370, 1: 192, -1: 113}) total:  675\n",
      "misterlonely  :  Counter({0: 16, 1: 16, -1: 6}) total:  38\n",
      "totalrecall2012rerelease  :  Counter({-1: 635, 1: 253, 0: 111}) total:  999\n",
      "avatar  :  Counter({1: 467, 0: 287, -1: 246}) total:  1000\n",
      "jurassicpark4  :  Counter({0: 481, 1: 334, -1: 185}) total:  1000\n"
     ]
    }
   ],
   "source": [
    "# for title, movie in fetched_movie.items():\n",
    "#     counter = Counter(movie['sentiment'])\n",
    "#     print title, \" : \", counter, 'total: ', sum(counter.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "Current Idea: Extract features from raw text but also try to reduce feature space.\n",
    "1. Create parse tree and lemmatize each tweet.\n",
    "2. Keep nouns, adjectives, and verbs.\n",
    "3. Create word count, DF-IDF, sentiment, and length features\n",
    "4. (If needed) use latent semantics analysis (i.e. Trucated SVD)\n",
    "5. Add gaussian noise to response values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "directories = glob('data2/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avatar = pd.read_csv(\"data2/avatar\", sep='\",\"', engine='python')\n",
    "avenger = pd.read_csv(\"data2/avengers2\", sep='\",\"', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    _text = text.lower()\n",
    "    for char in string.punctuation:\n",
    "        _text = _text.replace(char, \" \")\n",
    "    return _text\n",
    "\n",
    "# create features for the 'text' column in df\n",
    "def get_features(df):\n",
    "    trees = [parsetree(clean(text), lemmata=True)[0] for text in df['text']]\n",
    "    processed_tweets = df.assign(text=[\n",
    "        [word.lemma for word in tree if word.tag.startswith(('JJ', 'NN', 'VB', '!'))] # only keep verbs, noun, adjective\n",
    "        for tree in trees\n",
    "    ])\n",
    "    rare_words = get_rare_words(processed_tweets) # get rare words\n",
    "    lengths = [len(tweet) for tweet in processed_tweets['text']] # get length of words remaining\n",
    "    sentiments = [sentiment(tweet)[0] for tweet in processed_tweets['text']] # get sentiment -1.0 ~ 1.0\n",
    "    # create TfidfVectorizer, CountVectorizer and corresponding feature matrix\n",
    "    tfidf, X_tfidf, count, X_count = create_features(processed_tweets, rare_words) \n",
    "    return tfidf, X_tfidf, count, X_count, lengths, sentiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine multiple data frames into one by combining all texts in a data frame into one string\n",
    "def combine_df(dfs):\n",
    "    X = pd.DataFrame()\n",
    "    text = []\n",
    "    for df in dfs:\n",
    "        text.append(' '.join(df['text']))\n",
    "    return X.assign(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_df = combine_df([avatar, avenger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = get_features(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w... u'\\u201din', u'\\u2193', u'\\u2202', u'\\u30c4', u'\\u672c\\u65e5\\u306e\\u5e30\\u5b85song', u'\\U000fe330'],\n",
       "         strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "         vocabulary=None),\n",
       " <2x1278 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 1741 stored elements in Compressed Sparse Row format>,\n",
       " CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None,\n",
       "         stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w... u'\\u201din', u'\\u2193', u'\\u2202', u'\\u30c4', u'\\u672c\\u65e5\\u306e\\u5e30\\u5b85song', u'\\U000fe330'],\n",
       "         strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " <2x1278 sparse matrix of type '<type 'numpy.int64'>'\n",
       " \twith 1741 stored elements in Compressed Sparse Row format>,\n",
       " [10580, 11678],\n",
       " [0.26919640338978407, 0.1709612706134443])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=10, n_iter=7,\n",
       "       random_state=42, tol=0.0)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Latent Semantic Analysis (Trucated SVD) reduce dimension of feature matrix\n",
    "svd_tfidf = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "svd_count = TruncatedSVD(n_components=10, n_iter=7, random_state=42)\n",
    "svd_tfidf.fit(features[1])\n",
    "svd_count.fit(features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.246553014529\n",
      "0.385183379497\n"
     ]
    }
   ],
   "source": [
    "# percentage of variance explained\n",
    "print(svd_tfidf.explained_variance_ratio_.sum()) \n",
    "print(svd_count.explained_variance_ratio_.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "1. Linear Regression\n",
    "2. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "Asur, Sitaram, and Bernardo A. Huberman. \"Predicting the future with social media.\" Web Intelligence and Intelligent Agent Technology (WI-IAT), 2010 IEEE/WIC/ACM International Conference on. Vol. 1. IEEE, 2010.\n",
    "\n",
    "Bermingham, Adam, and Alan F. Smeaton. \"On using Twitter to monitor political sentiment and predict election results.\" (2011).\n",
    "\n",
    "Bollen, Johan, Huina Mao, and Xiaojun Zeng. \"Twitter mood predicts the stock market.\" Journal of Computational Science 2.1 (2011): 1-8.\n",
    "\n",
    "De Vries, Lisette, Sonja Gensler, and Peter SH Leeflang. \"Popularity of brand posts on brand fan pages: An investigation of the effects of social media marketing.\" Journal of Interactive Marketing 26.2 (2012): 83-91.\n",
    "\n",
    "Oghina, Andrei, et al. \"Predicting imdb movie ratings using social media.\" European Conference on Information Retrieval. Springer Berlin Heidelberg, 2012."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
